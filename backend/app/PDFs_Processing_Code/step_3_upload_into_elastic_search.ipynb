{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 把刚刚做的 embeddings 的文件上传到 elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 10:39:20,934 - DEBUG - Starting new HTTP connection (1): localhost:9200\n",
      "2025-01-03 10:39:20,964 - DEBUG - http://localhost:9200 \"HEAD / HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:20,965 - INFO - HEAD http://localhost:9200/ [status:200 duration:0.031s]\n",
      "2025-01-03 10:39:20,966 - INFO - Loading embeddings from D:\\Dropbox\\29. Ampelos\\24_PED\\PED_PITT_Aaron\\backend\\PDFs_Share\\pdf_json_output\\scd_entities_relationships_total_deduplicated_with_embeddings.json\n",
      "2025-01-03 10:39:24,022 - INFO - Loaded 2131 source documents\n",
      "2025-01-03 10:39:24,023 - INFO - Total relationships across all documents: 0\n",
      "2025-01-03 10:39:24,027 - DEBUG - http://localhost:9200 \"HEAD /ped_literature_3_files_01_01_2025 HTTP/1.1\" 404 0\n",
      "2025-01-03 10:39:24,028 - INFO - HEAD http://localhost:9200/ped_literature_3_files_01_01_2025 [status:404 duration:0.004s]\n",
      "2025-01-03 10:39:24,795 - DEBUG - http://localhost:9200 \"PUT /ped_literature_3_files_01_01_2025 HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:24,796 - INFO - PUT http://localhost:9200/ped_literature_3_files_01_01_2025 [status:200 duration:0.767s]\n",
      "2025-01-03 10:39:24,797 - INFO - Created index 'ped_literature_3_files_01_01_2025' with mapping\n",
      "Indexing documents:   0%|          | 0/22 [00:00<?, ?it/s]2025-01-03 10:39:25,462 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:25,463 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.496s]\n",
      "2025-01-03 10:39:25,465 - DEBUG - Successfully indexed batch 1\n",
      "Indexing documents:   5%|▍         | 1/22 [00:00<00:13,  1.51it/s]2025-01-03 10:39:25,902 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:25,903 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.274s]\n",
      "2025-01-03 10:39:25,906 - DEBUG - Successfully indexed batch 2\n",
      "Indexing documents:   9%|▉         | 2/22 [00:01<00:10,  1.88it/s]2025-01-03 10:39:26,327 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:26,328 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.267s]\n",
      "2025-01-03 10:39:26,330 - DEBUG - Successfully indexed batch 3\n",
      "Indexing documents:  14%|█▎        | 3/22 [00:01<00:09,  2.07it/s]2025-01-03 10:39:26,823 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:26,824 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.336s]\n",
      "2025-01-03 10:39:26,825 - DEBUG - Successfully indexed batch 4\n",
      "Indexing documents:  18%|█▊        | 4/22 [00:02<00:08,  2.05it/s]2025-01-03 10:39:27,264 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:27,265 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.270s]\n",
      "2025-01-03 10:39:27,266 - DEBUG - Successfully indexed batch 5\n",
      "Indexing documents:  23%|██▎       | 5/22 [00:02<00:08,  2.12it/s]2025-01-03 10:39:27,709 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:27,710 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.285s]\n",
      "2025-01-03 10:39:27,713 - DEBUG - Successfully indexed batch 6\n",
      "Indexing documents:  27%|██▋       | 6/22 [00:02<00:07,  2.16it/s]2025-01-03 10:39:28,150 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:28,152 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.263s]\n",
      "2025-01-03 10:39:28,153 - DEBUG - Successfully indexed batch 7\n",
      "Indexing documents:  32%|███▏      | 7/22 [00:03<00:06,  2.19it/s]2025-01-03 10:39:28,590 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:28,591 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.271s]\n",
      "2025-01-03 10:39:28,592 - DEBUG - Successfully indexed batch 8\n",
      "Indexing documents:  36%|███▋      | 8/22 [00:03<00:06,  2.22it/s]2025-01-03 10:39:29,087 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:29,089 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.322s]\n",
      "2025-01-03 10:39:29,092 - DEBUG - Successfully indexed batch 9\n",
      "Indexing documents:  41%|████      | 9/22 [00:04<00:06,  2.15it/s]2025-01-03 10:39:29,514 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:29,515 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.253s]\n",
      "2025-01-03 10:39:29,517 - DEBUG - Successfully indexed batch 10\n",
      "Indexing documents:  45%|████▌     | 10/22 [00:04<00:05,  2.21it/s]2025-01-03 10:39:29,950 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:29,951 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.277s]\n",
      "2025-01-03 10:39:29,953 - DEBUG - Successfully indexed batch 11\n",
      "Indexing documents:  50%|█████     | 11/22 [00:05<00:04,  2.23it/s]2025-01-03 10:39:30,454 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:30,455 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.321s]\n",
      "2025-01-03 10:39:30,457 - DEBUG - Successfully indexed batch 12\n",
      "Indexing documents:  55%|█████▍    | 12/22 [00:05<00:04,  2.15it/s]2025-01-03 10:39:30,875 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:30,877 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.238s]\n",
      "2025-01-03 10:39:30,878 - DEBUG - Successfully indexed batch 13\n",
      "Indexing documents:  59%|█████▉    | 13/22 [00:06<00:04,  2.21it/s]2025-01-03 10:39:31,295 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:31,296 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.258s]\n",
      "2025-01-03 10:39:31,299 - DEBUG - Successfully indexed batch 14\n",
      "Indexing documents:  64%|██████▎   | 14/22 [00:06<00:03,  2.26it/s]2025-01-03 10:39:31,745 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:31,749 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.234s]\n",
      "2025-01-03 10:39:31,755 - DEBUG - Successfully indexed batch 15\n",
      "Indexing documents:  68%|██████▊   | 15/22 [00:06<00:03,  2.23it/s]2025-01-03 10:39:32,199 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:32,200 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.254s]\n",
      "2025-01-03 10:39:32,202 - DEBUG - Successfully indexed batch 16\n",
      "Indexing documents:  73%|███████▎  | 16/22 [00:07<00:02,  2.24it/s]2025-01-03 10:39:32,624 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:32,624 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.263s]\n",
      "2025-01-03 10:39:32,626 - DEBUG - Successfully indexed batch 17\n",
      "Indexing documents:  77%|███████▋  | 17/22 [00:07<00:02,  2.27it/s]2025-01-03 10:39:33,044 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:33,045 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.256s]\n",
      "2025-01-03 10:39:33,047 - DEBUG - Successfully indexed batch 18\n",
      "Indexing documents:  82%|████████▏ | 18/22 [00:08<00:01,  2.31it/s]2025-01-03 10:39:33,484 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:33,485 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.281s]\n",
      "2025-01-03 10:39:33,487 - DEBUG - Successfully indexed batch 19\n",
      "Indexing documents:  86%|████████▋ | 19/22 [00:08<00:01,  2.30it/s]2025-01-03 10:39:33,923 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:33,924 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.265s]\n",
      "2025-01-03 10:39:33,926 - DEBUG - Successfully indexed batch 20\n",
      "Indexing documents:  91%|█████████ | 20/22 [00:09<00:00,  2.29it/s]2025-01-03 10:39:34,391 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:34,392 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.279s]\n",
      "2025-01-03 10:39:34,393 - DEBUG - Successfully indexed batch 21\n",
      "Indexing documents:  95%|█████████▌| 21/22 [00:09<00:00,  2.24it/s]2025-01-03 10:39:34,588 - DEBUG - http://localhost:9200 \"PUT /_bulk?refresh=true HTTP/1.1\" 200 0\n",
      "2025-01-03 10:39:34,589 - INFO - PUT http://localhost:9200/_bulk?refresh=true [status:200 duration:0.137s]\n",
      "2025-01-03 10:39:34,590 - DEBUG - Successfully indexed batch 22\n",
      "Indexing documents: 100%|██████████| 22/22 [00:09<00:00,  2.25it/s]\n",
      "2025-01-03 10:39:34,593 - INFO - Successfully indexed 2131 documents\n",
      "2025-01-03 10:39:34,601 - DEBUG - http://localhost:9200 \"GET /ped_literature_3_files_01_01_2025/_stats HTTP/1.1\" 200 None\n",
      "2025-01-03 10:39:34,602 - INFO - GET http://localhost:9200/ped_literature_3_files_01_01_2025/_stats [status:200 duration:0.009s]\n",
      "2025-01-03 10:39:34,618 - DEBUG - http://localhost:9200 \"POST /ped_literature_3_files_01_01_2025/_search HTTP/1.1\" 200 None\n",
      "2025-01-03 10:39:34,619 - INFO - POST http://localhost:9200/ped_literature_3_files_01_01_2025/_search [status:200 duration:0.016s]\n",
      "2025-01-03 10:39:34,622 - INFO - Sample document contains 0 relationships\n",
      "2025-01-03 10:39:34,623 - INFO - Index 'ped_literature_3_files_01_01_2025' contains 2131 documents\n",
      "2025-01-03 10:39:34,675 - INFO - Processing complete: {'success': True, 'documents_indexed': 2131, 'total_relationships': 0, 'index_name': 'ped_literature_3_files_01_01_2025'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from elasticsearch import Elasticsearch\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class ElasticsearchUploader:\n",
    "    def __init__(self, \n",
    "                 es_host: str = 'localhost',\n",
    "                 es_port: int = 9200,\n",
    "                 index_name: str = 'ped_literature_3_files_01_01_2025'):\n",
    "        \"\"\"Initialize Elasticsearch connection.\"\"\"\n",
    "        self.es = Elasticsearch(\n",
    "            hosts=[f\"http://{es_host}:{es_port}\"],\n",
    "            basic_auth=('elastic', os.getenv('ES_PASSWORD', 'Lyx19930115'))\n",
    "        )\n",
    "        self.index_name = index_name\n",
    "\n",
    "    def create_es_mapping(self):\n",
    "        \"\"\"Create Elasticsearch index with appropriate mapping.\"\"\"\n",
    "        mapping = {\n",
    "            \"settings\": {\n",
    "                \"number_of_shards\": 1,\n",
    "                \"number_of_replicas\": 1\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"source\": {\n",
    "                        \"properties\": {\n",
    "                            \"title\": {\"type\": \"text\"},\n",
    "                            \"page_number\": {\"type\": \"integer\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"original_text\": {\"type\": \"text\"},\n",
    "                    \"embedding\": {\n",
    "                        \"type\": \"dense_vector\",\n",
    "                        \"dims\": 1536,\n",
    "                        \"similarity\": \"cosine\"\n",
    "                    },\n",
    "                    \"relationships\": {\n",
    "                        \"type\": \"nested\",\n",
    "                        \"properties\": {\n",
    "                            \"subject\": {\"type\": \"keyword\"},\n",
    "                            \"predicate\": {\"type\": \"keyword\"},\n",
    "                            \"object\": {\"type\": \"keyword\"}\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Delete index if it exists\n",
    "            if self.es.indices.exists(index=self.index_name):\n",
    "                logger.info(f\"Deleting existing index '{self.index_name}'\")\n",
    "                self.es.indices.delete(index=self.index_name)\n",
    "            \n",
    "            # Create new index\n",
    "            self.es.indices.create(index=self.index_name, body=mapping)\n",
    "            logger.info(f\"Created index '{self.index_name}' with mapping\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error creating index mapping: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def bulk_index_documents(self, documents: List[Dict[str, Any]], batch_size: int = 100):\n",
    "        \"\"\"Index documents in bulk to Elasticsearch.\"\"\"\n",
    "        try:\n",
    "            total_batches = (len(documents) + batch_size - 1) // batch_size\n",
    "            \n",
    "            for i in tqdm(range(0, len(documents), batch_size), desc=\"Indexing documents\", total=total_batches):\n",
    "                batch = documents[i:i + batch_size]\n",
    "                bulk_data = []\n",
    "                \n",
    "                for doc in batch:\n",
    "                    # Add index action\n",
    "                    bulk_data.extend([\n",
    "                        {\"index\": {\"_index\": self.index_name}},\n",
    "                        doc\n",
    "                    ])\n",
    "                \n",
    "                if bulk_data:\n",
    "                    response = self.es.bulk(operations=bulk_data, refresh=True)\n",
    "                    \n",
    "                    # Check for errors\n",
    "                    if response.get('errors', False):\n",
    "                        logger.warning(f\"Some errors occurred in batch {i//batch_size + 1}\")\n",
    "                        for item in response['items']:\n",
    "                            if 'error' in item.get('index', {}):\n",
    "                                logger.error(f\"Error: {item['index']['error']}\")\n",
    "                    else:\n",
    "                        logger.debug(f\"Successfully indexed batch {i//batch_size + 1}\")\n",
    "            \n",
    "            logger.info(f\"Successfully indexed {len(documents)} documents\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during bulk indexing: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def verify_index(self):\n",
    "        \"\"\"Verify the index exists and return basic stats.\"\"\"\n",
    "        try:\n",
    "            stats = self.es.indices.stats(index=self.index_name)\n",
    "            doc_count = stats['indices'][self.index_name]['total']['docs']['count']\n",
    "            \n",
    "            # Get a sample document to verify structure\n",
    "            sample = self.es.search(\n",
    "                index=self.index_name,\n",
    "                size=1,\n",
    "                query={\"match_all\": {}}\n",
    "            )\n",
    "            if sample['hits']['hits']:\n",
    "                rel_count = len(sample['hits']['hits'][0]['_source'].get('relationships', []))\n",
    "                logger.info(f\"Sample document contains {rel_count} relationships\")\n",
    "            \n",
    "            logger.info(f\"Index '{self.index_name}' contains {doc_count} documents\")\n",
    "            return doc_count\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error verifying index: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def process_embeddings_file(self, input_path: str, batch_size: int = 100):\n",
    "        \"\"\"Process the embeddings file and upload to Elasticsearch.\"\"\"\n",
    "        try:\n",
    "            # Load embeddings file\n",
    "            logger.info(f\"Loading embeddings from {input_path}\")\n",
    "            with open(input_path, 'r', encoding='utf-8') as f:\n",
    "                documents = json.load(f)\n",
    "            \n",
    "            logger.info(f\"Loaded {len(documents)} source documents\")\n",
    "            total_relationships = sum(len(doc.get('relationships', [])) for doc in documents)\n",
    "            logger.info(f\"Total relationships across all documents: {total_relationships}\")\n",
    "            \n",
    "            # Create/recreate index with mapping\n",
    "            self.create_es_mapping()\n",
    "            \n",
    "            # Upload documents\n",
    "            self.bulk_index_documents(documents, batch_size)\n",
    "            \n",
    "            # Verify upload\n",
    "            final_count = self.verify_index()\n",
    "            \n",
    "            return {\n",
    "                \"success\": True,\n",
    "                \"documents_indexed\": final_count,\n",
    "                \"total_relationships\": total_relationships,\n",
    "                \"index_name\": self.index_name\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing embeddings file: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def delete_index(self):\n",
    "        \"\"\"Delete the specified index if it exists.\"\"\"\n",
    "        try:\n",
    "            if self.es.indices.exists(index=self.index_name):\n",
    "                self.es.indices.delete(index=self.index_name)\n",
    "                logger.info(f\"Successfully deleted index '{self.index_name}'\")\n",
    "            else:\n",
    "                logger.info(f\"Index '{self.index_name}' does not exist\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error deleting index: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to upload embeddings to Elasticsearch.\"\"\"\n",
    "    try:\n",
    "        uploader = ElasticsearchUploader()\n",
    "        \n",
    "        # Verify Elasticsearch connection\n",
    "        if not uploader.es.ping():\n",
    "            raise ConnectionError(\"Could not connect to Elasticsearch\")\n",
    "            \n",
    "        input_path = r\"D:\\Dropbox\\29. Ampelos\\24_PED\\PED_PITT_Aaron\\backend\\PDFs_Share\\pdf_json_output\\scd_entities_relationships_total_deduplicated_with_embeddings.json\"\n",
    "        \n",
    "        # Process and upload\n",
    "        result = uploader.process_embeddings_file(input_path)\n",
    "        logger.info(f\"Processing complete: {result}\")\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main: {str(e)}\")\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 利用刚刚做的带有 embeddings 的文件进行查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 10:41:04,916 - DEBUG - Starting new HTTP connection (1): localhost:9200\n",
      "2025-01-03 10:41:04,924 - DEBUG - http://localhost:9200 \"HEAD / HTTP/1.1\" 200 0\n",
      "2025-01-03 10:41:04,926 - INFO - HEAD http://localhost:9200/ [status:200 duration:0.010s]\n",
      "2025-01-03 10:41:05,574 - INFO - Successfully initialized ElasticsearchQuerier with index: ped_literature_3_files_01_01_2025\n",
      "2025-01-03 10:41:05,613 - DEBUG - http://localhost:9200 \"POST /ped_literature_3_files_01_01_2025/_search HTTP/1.1\" 200 None\n",
      "2025-01-03 10:41:05,616 - INFO - POST http://localhost:9200/ped_literature_3_files_01_01_2025/_search [status:200 duration:0.035s]\n",
      "2025-01-03 10:41:05,623 - DEBUG - Keyword search completed for query: What are the treatments for sickle cell pain crisis?\n",
      "2025-01-03 10:41:05,630 - DEBUG - Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000020F0D507B00>, 'json_data': {'input': 'What are the treatments for sickle cell pain crisis?', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      "2025-01-03 10:41:05,633 - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "2025-01-03 10:41:05,635 - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2025-01-03 10:41:05,668 - DEBUG - connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020F0D445190>\n",
      "2025-01-03 10:41:05,670 - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x0000020F0D33BE50> server_hostname='api.openai.com' timeout=5.0\n",
      "2025-01-03 10:41:05,697 - DEBUG - start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000020F0C1989E0>\n",
      "2025-01-03 10:41:05,699 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-01-03 10:41:05,700 - DEBUG - send_request_headers.complete\n",
      "2025-01-03 10:41:05,701 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-01-03 10:41:05,703 - DEBUG - send_request_body.complete\n",
      "2025-01-03 10:41:05,704 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keyword Search Results for: 'What are the treatments for sickle cell pain crisis?'\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 21.634):\n",
      "Source: Caring for Your Baby and Young Child  Birth to Age 5 (Tanya Altmann American Academy of Pediatrics) (Z-Library), Page 661\n",
      "Text Preview: 638 638 CHRonIC He ALtH Cond ItIons A nd dI seAses disorders in the SCD complex have similar symptoms, such...\n",
      "\n",
      "Relationships:\n",
      "  1. N/A → N/A → N/A\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 2 (Score: 20.148):\n",
      "Source: Caring for Your Baby and Young Child  Birth to Age 5 (Tanya Altmann American Academy of Pediatrics) (Z-Library), Page 662\n",
      "Text Preview: 639 639 sICKL e Ce LL dI seAse Treatment If your child has SCD, she should be diagnosed as early...\n",
      "\n",
      "Relationships:\n",
      "  1. N/A → N/A → N/A\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 3 (Score: 16.418):\n",
      "Source: Bright Futures Guidelines for Health... (Z-Library), Page 397\n",
      "Text Preview: Table 2 reviews the 3 types of heat-related illness as well as exertional sickling. Ensuring Adequate Nutrition To perform optimally...\n",
      "\n",
      "Relationships:\n",
      "  1. N/A → N/A → N/A\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 10:41:05,877 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 03 Jan 2025 15:41:05 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-pztuwilaiwjvfjkapn4ghvrs'), (b'openai-processing-ms', b'66'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'4999986'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_a3479a83a8a15b08e014c8e3bcd4df7c'), (b'CF-Cache-Status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=yDfKX7qgHmzi5QX0G19DElOQAP69S8KlGbR8SpMStLg-1735918865-1.0.1.1-k6xUAEsN5jNBXvdfiFy09hlSECPjaYgDE0gZtZaUx1BeoKQsTQ.L.vGOdo8jC1iSvZ0MdfiQ7WLBAAbUrcwlNw; path=/; expires=Fri, 03-Jan-25 16:11:05 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=UgVmo2kLYXXdiO7BZNFVZqFWxc.8zJ_aU6rURiuGGl8-1735918865898-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8fc4174eecc33045-BOS'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n",
      "2025-01-03 10:41:05,882 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-01-03 10:41:05,884 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-01-03 10:41:05,889 - DEBUG - receive_response_body.complete\n",
      "2025-01-03 10:41:05,892 - DEBUG - response_closed.started\n",
      "2025-01-03 10:41:05,894 - DEBUG - response_closed.complete\n",
      "2025-01-03 10:41:05,895 - DEBUG - HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers([('date', 'Fri, 03 Jan 2025 15:41:05 GMT'), ('content-type', 'application/json'), ('transfer-encoding', 'chunked'), ('connection', 'keep-alive'), ('access-control-allow-origin', '*'), ('access-control-expose-headers', 'X-Request-ID'), ('openai-model', 'text-embedding-ada-002'), ('openai-organization', 'user-pztuwilaiwjvfjkapn4ghvrs'), ('openai-processing-ms', '66'), ('openai-version', '2020-10-01'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('x-ratelimit-limit-requests', '10000'), ('x-ratelimit-limit-tokens', '5000000'), ('x-ratelimit-remaining-requests', '9999'), ('x-ratelimit-remaining-tokens', '4999986'), ('x-ratelimit-reset-requests', '6ms'), ('x-ratelimit-reset-tokens', '0s'), ('x-request-id', 'req_a3479a83a8a15b08e014c8e3bcd4df7c'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=yDfKX7qgHmzi5QX0G19DElOQAP69S8KlGbR8SpMStLg-1735918865-1.0.1.1-k6xUAEsN5jNBXvdfiFy09hlSECPjaYgDE0gZtZaUx1BeoKQsTQ.L.vGOdo8jC1iSvZ0MdfiQ7WLBAAbUrcwlNw; path=/; expires=Fri, 03-Jan-25 16:11:05 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=UgVmo2kLYXXdiO7BZNFVZqFWxc.8zJ_aU6rURiuGGl8-1735918865898-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '8fc4174eecc33045-BOS'), ('content-encoding', 'gzip'), ('alt-svc', 'h3=\":443\"; ma=86400')])\n",
      "2025-01-03 10:41:05,897 - DEBUG - request_id: req_a3479a83a8a15b08e014c8e3bcd4df7c\n",
      "2025-01-03 10:41:05,900 - DEBUG - Successfully generated embedding for text: What are the treatments for sickle cell pain crisi...\n",
      "2025-01-03 10:41:05,938 - DEBUG - http://localhost:9200 \"POST /ped_literature_3_files_01_01_2025/_search HTTP/1.1\" 200 None\n",
      "2025-01-03 10:41:05,939 - INFO - POST http://localhost:9200/ped_literature_3_files_01_01_2025/_search [status:200 duration:0.035s]\n",
      "2025-01-03 10:41:05,947 - DEBUG - Semantic search completed for query: What are the treatments for sickle cell pain crisis?\n",
      "2025-01-03 10:41:05,950 - DEBUG - Request options: {'method': 'post', 'url': '/embeddings', 'files': None, 'post_parser': <function Embeddings.create.<locals>.parser at 0x0000020F0D505300>, 'json_data': {'input': 'What are the treatments for sickle cell pain crisis?', 'model': 'text-embedding-ada-002', 'encoding_format': 'base64'}}\n",
      "2025-01-03 10:41:05,953 - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/embeddings\n",
      "2025-01-03 10:41:05,955 - DEBUG - send_request_headers.started request=<Request [b'POST']>\n",
      "2025-01-03 10:41:05,956 - DEBUG - send_request_headers.complete\n",
      "2025-01-03 10:41:05,957 - DEBUG - send_request_body.started request=<Request [b'POST']>\n",
      "2025-01-03 10:41:05,959 - DEBUG - send_request_body.complete\n",
      "2025-01-03 10:41:05,960 - DEBUG - receive_response_headers.started request=<Request [b'POST']>\n",
      "2025-01-03 10:41:06,137 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Fri, 03 Jan 2025 15:41:06 GMT'), (b'Content-Type', b'application/json'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'access-control-allow-origin', b'*'), (b'access-control-expose-headers', b'X-Request-ID'), (b'openai-model', b'text-embedding-ada-002'), (b'openai-organization', b'user-pztuwilaiwjvfjkapn4ghvrs'), (b'openai-processing-ms', b'75'), (b'openai-version', b'2020-10-01'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'x-ratelimit-limit-requests', b'10000'), (b'x-ratelimit-limit-tokens', b'5000000'), (b'x-ratelimit-remaining-requests', b'9999'), (b'x-ratelimit-remaining-tokens', b'4999986'), (b'x-ratelimit-reset-requests', b'6ms'), (b'x-ratelimit-reset-tokens', b'0s'), (b'x-request-id', b'req_094a55087e6faa2771e029b6c0a06753'), (b'CF-Cache-Status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'8fc417507ef23045-BOS'), (b'Content-Encoding', b'gzip'), (b'alt-svc', b'h3=\":443\"; ma=86400')])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Semantic Search Results for: 'What are the treatments for sickle cell pain crisis?'\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 1.873):\n",
      "Source: Caring for Your Baby and Young Child  Birth to Age 5 (Tanya Altmann American Academy of Pediatrics) (Z-Library), Page 662\n",
      "Text Preview: 639 639 sICKL e Ce LL dI seAse Treatment If your child has SCD, she should be diagnosed as early...\n",
      "\n",
      "Relationships:\n",
      "  1. N/A → N/A → N/A\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 2 (Score: 1.822):\n",
      "Source: Bright Futures Guidelines for Health... (Z-Library), Page 397\n",
      "Text Preview: Table 2 reviews the 3 types of heat-related illness as well as exertional sickling. Ensuring Adequate Nutrition To perform optimally...\n",
      "\n",
      "Relationships:\n",
      "  1. N/A → N/A → N/A\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 3 (Score: 1.804):\n",
      "Source: Caring for Your Baby and Young Child  Birth to Age 5 (Tanya Altmann American Academy of Pediatrics) (Z-Library), Page 661\n",
      "Text Preview: 638 638 CHRonIC He ALtH Cond ItIons A nd dI seAses disorders in the SCD complex have similar symptoms, such...\n",
      "\n",
      "Relationships:\n",
      "  1. N/A → N/A → N/A\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-03 10:41:06,139 - INFO - HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-01-03 10:41:06,141 - DEBUG - receive_response_body.started request=<Request [b'POST']>\n",
      "2025-01-03 10:41:06,179 - DEBUG - receive_response_body.complete\n",
      "2025-01-03 10:41:06,181 - DEBUG - response_closed.started\n",
      "2025-01-03 10:41:06,183 - DEBUG - response_closed.complete\n",
      "2025-01-03 10:41:06,185 - DEBUG - HTTP Response: POST https://api.openai.com/v1/embeddings \"200 OK\" Headers({'date': 'Fri, 03 Jan 2025 15:41:06 GMT', 'content-type': 'application/json', 'transfer-encoding': 'chunked', 'connection': 'keep-alive', 'access-control-allow-origin': '*', 'access-control-expose-headers': 'X-Request-ID', 'openai-model': 'text-embedding-ada-002', 'openai-organization': 'user-pztuwilaiwjvfjkapn4ghvrs', 'openai-processing-ms': '75', 'openai-version': '2020-10-01', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-ratelimit-limit-requests': '10000', 'x-ratelimit-limit-tokens': '5000000', 'x-ratelimit-remaining-requests': '9999', 'x-ratelimit-remaining-tokens': '4999986', 'x-ratelimit-reset-requests': '6ms', 'x-ratelimit-reset-tokens': '0s', 'x-request-id': 'req_094a55087e6faa2771e029b6c0a06753', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '8fc417507ef23045-BOS', 'content-encoding': 'gzip', 'alt-svc': 'h3=\":443\"; ma=86400'})\n",
      "2025-01-03 10:41:06,186 - DEBUG - request_id: req_094a55087e6faa2771e029b6c0a06753\n",
      "2025-01-03 10:41:06,187 - DEBUG - Successfully generated embedding for text: What are the treatments for sickle cell pain crisi...\n",
      "2025-01-03 10:41:06,223 - DEBUG - http://localhost:9200 \"POST /ped_literature_3_files_01_01_2025/_search HTTP/1.1\" 200 None\n",
      "2025-01-03 10:41:06,226 - INFO - POST http://localhost:9200/ped_literature_3_files_01_01_2025/_search [status:200 duration:0.035s]\n",
      "2025-01-03 10:41:06,232 - DEBUG - Hybrid search completed for query: What are the treatments for sickle cell pain crisis?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hybrid Search Results for: 'What are the treatments for sickle cell pain crisis?'\n",
      "================================================================================\n",
      "\n",
      "Result 1 (Score: 7.053):\n",
      "Source: Caring for Your Baby and Young Child  Birth to Age 5 (Tanya Altmann American Academy of Pediatrics) (Z-Library), Page 661\n",
      "Text Preview: 638 638 CHRonIC He ALtH Cond ItIons A nd dI seAses disorders in the SCD complex have similar symptoms, such...\n",
      "\n",
      "Relationships:\n",
      "  1. N/A → N/A → N/A\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 2 (Score: 6.655):\n",
      "Source: Caring for Your Baby and Young Child  Birth to Age 5 (Tanya Altmann American Academy of Pediatrics) (Z-Library), Page 662\n",
      "Text Preview: 639 639 sICKL e Ce LL dI seAse Treatment If your child has SCD, she should be diagnosed as early...\n",
      "\n",
      "Relationships:\n",
      "  1. N/A → N/A → N/A\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Result 3 (Score: 5.501):\n",
      "Source: Bright Futures Guidelines for Health... (Z-Library), Page 397\n",
      "Text Preview: Table 2 reviews the 3 types of heat-related illness as well as exertional sickling. Ensuring Adequate Nutrition To perform optimally...\n",
      "\n",
      "Relationships:\n",
      "  1. N/A → N/A → N/A\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "from elasticsearch import Elasticsearch\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import traceback\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "class ElasticsearchQuerier:\n",
    "    def __init__(self,\n",
    "                 es_host: str = 'localhost',\n",
    "                 es_port: int = 9200,\n",
    "                 index_name: str = 'ped_literature_3_files_01_01_2025'):\n",
    "        \"\"\"Initialize Elasticsearch connection and OpenAI client.\"\"\"\n",
    "        try:\n",
    "            self.es = Elasticsearch(\n",
    "                hosts=[f\"http://{es_host}:{es_port}\"],\n",
    "                basic_auth=('elastic', os.getenv('ES_PASSWORD', 'Lyx19930115'))\n",
    "            )\n",
    "            if not self.es.ping():\n",
    "                raise ConnectionError(\"Could not connect to Elasticsearch\")\n",
    "            \n",
    "            self.openai_client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "            if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "                raise ValueError(\"OpenAI API key not found in environment variables\")\n",
    "                \n",
    "            self.index_name = index_name\n",
    "            logger.info(f\"Successfully initialized ElasticsearchQuerier with index: {index_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error initializing ElasticsearchQuerier: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_embedding(self, text: str) -> list:\n",
    "        \"\"\"Get embedding for a query text.\"\"\"\n",
    "        try:\n",
    "            response = self.openai_client.embeddings.create(\n",
    "                model=\"text-embedding-ada-002\",\n",
    "                input=text\n",
    "            )\n",
    "            logger.debug(f\"Successfully generated embedding for text: {text[:50]}...\")\n",
    "            return response.data[0].embedding\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating embedding: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def get_first_n_words(self, text: str, n: int = 20) -> str:\n",
    "        \"\"\"Get first n words from a text.\"\"\"\n",
    "        try:\n",
    "            words = text.split()\n",
    "            return ' '.join(words[:n]) + ('...' if len(words) > n else '')\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing text preview: {str(e)}\")\n",
    "            return text[:100] + '...'  # Fallback to character-based truncation\n",
    "\n",
    "    def keyword_search(self, query_text: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Basic keyword search focused only on original_text field.\"\"\"\n",
    "        try:\n",
    "            query = {\n",
    "                \"size\": top_k,\n",
    "                \"query\": {\n",
    "                    \"match\": {\n",
    "                        \"original_text\": {\n",
    "                            \"query\": query_text,\n",
    "                            \"operator\": \"or\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = self.es.search(index=self.index_name, body=query)\n",
    "            logger.debug(f\"Keyword search completed for query: {query_text}\")\n",
    "            return self._process_results(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in keyword search: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def semantic_search(self, query_text: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Enhanced semantic search with error handling.\"\"\"\n",
    "        try:\n",
    "            query_embedding = self.get_embedding(query_text)\n",
    "            \n",
    "            query = {\n",
    "                \"size\": top_k,\n",
    "                \"query\": {\n",
    "                    \"script_score\": {\n",
    "                        \"query\": {\"match_all\": {}},\n",
    "                        \"script\": {\n",
    "                            \"source\": \"cosineSimilarity(params.query_vector, 'embedding') + 1.0\",\n",
    "                            \"params\": {\"query_vector\": query_embedding}\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = self.es.search(index=self.index_name, body=query)\n",
    "            logger.debug(f\"Semantic search completed for query: {query_text}\")\n",
    "            return self._process_results(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in semantic search: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def hybrid_search(self, query_text: str, top_k: int = 3) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Enhanced hybrid search combining original_text keyword search with semantic search.\"\"\"\n",
    "        try:\n",
    "            query_embedding = self.get_embedding(query_text)\n",
    "            \n",
    "            query = {\n",
    "                \"size\": top_k,\n",
    "                \"query\": {\n",
    "                    \"script_score\": {\n",
    "                        \"query\": {\n",
    "                            \"match\": {\n",
    "                                \"original_text\": {\n",
    "                                    \"query\": query_text,\n",
    "                                    \"operator\": \"or\"\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"script\": {\n",
    "                            \"source\": \"cosineSimilarity(params.query_vector, 'embedding') * params.semantic_weight + _score * params.keyword_weight\",\n",
    "                            \"params\": {\n",
    "                                \"query_vector\": query_embedding,\n",
    "                                \"semantic_weight\": 0.7,\n",
    "                                \"keyword_weight\": 0.3\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            response = self.es.search(index=self.index_name, body=query)\n",
    "            logger.debug(f\"Hybrid search completed for query: {query_text}\")\n",
    "            return self._process_results(response)\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in hybrid search: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _process_results(self, response: Dict[str, Any]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process search results grouping by source with relationships.\"\"\"\n",
    "        try:\n",
    "            # Dictionary to group results by source\n",
    "            grouped_results = {}\n",
    "            \n",
    "            for hit in response['hits']['hits']:\n",
    "                source = hit['_source']\n",
    "                source_info = source.get('source', {'title': 'Unknown', 'page_number': 0})\n",
    "                source_key = f\"{source_info.get('title', 'Unknown')}_{source_info.get('page_number', 0)}\"\n",
    "                \n",
    "                # If this source isn't in our grouped results yet, initialize it\n",
    "                if source_key not in grouped_results:\n",
    "                    grouped_results[source_key] = {\n",
    "                        'source': source_info,\n",
    "                        'text_preview': self.get_first_n_words(source.get('original_text', '')),\n",
    "                        'score': round(hit['_score'], 3),\n",
    "                        'relationships': []\n",
    "                    }\n",
    "                \n",
    "                # Add relationships for this source\n",
    "                relationships = source.get('relationships', [])\n",
    "                if relationships:\n",
    "                    for rel in relationships:\n",
    "                        relationship = {\n",
    "                            'subject': rel.get('subject', 'N/A'),\n",
    "                            'predicate': rel.get('predicate', 'N/A'),\n",
    "                            'object': rel.get('object', 'N/A')\n",
    "                        }\n",
    "                        if relationship not in grouped_results[source_key]['relationships']:\n",
    "                            grouped_results[source_key]['relationships'].append(relationship)\n",
    "                else:\n",
    "                    # Handle documents without relationships\n",
    "                    relationship = {\n",
    "                        'subject': 'N/A',\n",
    "                        'predicate': 'N/A',\n",
    "                        'object': 'N/A'\n",
    "                    }\n",
    "                    if relationship not in grouped_results[source_key]['relationships']:\n",
    "                        grouped_results[source_key]['relationships'].append(relationship)\n",
    "            \n",
    "            # Convert grouped results to list format\n",
    "            results = []\n",
    "            for source_data in grouped_results.values():\n",
    "                results.append(source_data)\n",
    "                \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing search results: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "def print_search_results(search_type: str, query: str, results: List[Dict[str, Any]]):\n",
    "        \"\"\"Enhanced results printing with source grouping.\"\"\"\n",
    "        try:\n",
    "            print(f\"\\n{search_type} Search Results for: '{query}'\")\n",
    "            print(\"=\" * 80)\n",
    "            \n",
    "            if not results:\n",
    "                print(\"No results found.\")\n",
    "                return\n",
    "                \n",
    "            for i, result in enumerate(results, 1):\n",
    "                print(f\"\\nResult {i} (Score: {result['score']}):\")\n",
    "                print(f\"Source: {result['source'].get('title', 'Unknown')}, \"\n",
    "                    f\"Page {result['source'].get('page_number', 'N/A')}\")\n",
    "                print(f\"Text Preview: {result['text_preview']}\")\n",
    "                print(\"\\nRelationships:\")\n",
    "                for j, rel in enumerate(result['relationships'], 1):\n",
    "                    print(f\"  {j}. {rel['subject']} → {rel['predicate']} → {rel['object']}\")\n",
    "                print(\"-\" * 80)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error printing search results: {str(e)}\")\n",
    "            print(\"Error occurred while displaying results.\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Enhanced main function with proper error handling.\"\"\"\n",
    "    try:\n",
    "        load_dotenv()\n",
    "        querier = ElasticsearchQuerier()\n",
    "        \n",
    "        query = \"What are the treatments for sickle cell pain crisis?\"\n",
    "        \n",
    "        # Run all three search types with error handling\n",
    "        for search_type in ['keyword', 'semantic', 'hybrid']:\n",
    "            try:\n",
    "                results = getattr(querier, f\"{search_type}_search\")(query)\n",
    "                print_search_results(search_type.capitalize(), query, results)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in {search_type} search: {str(e)}\")\n",
    "                print(f\"Error occurred during {search_type} search.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in main: {str(e)}\")\n",
    "        logger.error(f\"Traceback: {traceback.format_exc()}\")\n",
    "        raise\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ped",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
